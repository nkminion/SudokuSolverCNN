{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70e8635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using the GPU\n",
      "Found 1 GPUs\n",
      "GPU 0 found: NVIDIA GeForce RTX 5070 Laptop GPU\n",
      "Selected Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Importing packages and checking GPU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Dataset,random_split,ConcatDataset\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from PIL import Image,ImageDraw,ImageFont\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tprint(\"PyTorch is using the GPU\")\n",
    "\tGPUCount = torch.cuda.device_count()\n",
    "\tprint(f\"Found {GPUCount} GPUs\")\n",
    "\n",
    "\tfor i in range(GPUCount):\n",
    "\t\tprint(f\"GPU {i} found: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "\tdevice = torch.device(\"cuda:0\")\n",
    "else:\n",
    "\tprint(\"PyTorch is using the CPU\")\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Selected Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db59621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of EMNIST train data: 60000\n",
      "Size of EMNIST test data: 10000\n"
     ]
    }
   ],
   "source": [
    "#Load BuiltIn dataset\n",
    "\n",
    "Transform = transforms.Compose(\n",
    "\t[\n",
    "\t\ttransforms.Grayscale(num_output_channels=1),\n",
    "\t\ttransforms.ToTensor(),\n",
    "  \t\ttransforms.Normalize((0.5,),(0.5,)),\n",
    "\t\ttransforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0),\n",
    "\t\ttransforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1))\n",
    "\t])\n",
    "\n",
    "TrainSet = torchvision.datasets.MNIST(root='./data',train=True,download=True,transform=Transform)\n",
    "TestSet = torchvision.datasets.MNIST(root='./data',train=False,download=True,transform=Transform)\n",
    "\n",
    "print(f\"Size of EMNIST train data: {len(TrainSet)}\")\n",
    "print(f\"Size of EMNIST test data: {len(TestSet)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a162619b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of custom data: 45140\n"
     ]
    }
   ],
   "source": [
    "#Load Custom dataset\n",
    "\n",
    "FontPath = 'fonts'\n",
    "CustomImages = []\n",
    "CustomLabels = []\n",
    "\n",
    "ImagesDirectory = 'CustomDataset'\n",
    "\n",
    "CustomDataset = ImageFolder(root=ImagesDirectory , transform=Transform)\n",
    "\n",
    "print(f\"Size of custom data: {len(CustomDataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c5011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of custom data: 115140\n",
      "Size of train data: 80598\n",
      "Size of validation data: 23028\n",
      "Size of test data: 11514\n",
      "Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "#Combine datasets and split into new ones\n",
    "\n",
    "Dataset = ConcatDataset([TrainSet,TestSet,CustomDataset])\n",
    "\n",
    "print(f\"Size of custom data: {len(Dataset)}\")\n",
    "\n",
    "TrainSize = int(0.7*(len(Dataset)))\n",
    "ValSize = int(0.2*(len(Dataset)))\n",
    "TestSize = len(Dataset)-(TrainSize+ValSize)\n",
    "\n",
    "print(f\"Size of train data: {TrainSize}\")\n",
    "print(f\"Size of validation data: {ValSize}\")\n",
    "print(f\"Size of test data: {TestSize}\")\n",
    "\n",
    "TrainDataset, ValDataset, TestDataset = random_split(Dataset , [TrainSize, ValSize, TestSize])\n",
    "\n",
    "TrainLoader = DataLoader(TrainDataset,batch_size=128,shuffle=True)\n",
    "ValLoader = DataLoader(ValDataset,batch_size=128,shuffle=False)\n",
    "TestLoader = DataLoader(TestDataset,batch_size=128,shuffle=False)\n",
    "\n",
    "print(\"Dataset loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed12a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MNIST Model...\n",
      "Model Created\n",
      "Model Summary: \n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             288\n",
      "       BatchNorm2d-2           [-1, 32, 28, 28]              64\n",
      "              ReLU-3           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-4           [-1, 32, 14, 14]               0\n",
      "            Conv2d-5           [-1, 32, 14, 14]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 14, 14]              64\n",
      "              ReLU-7           [-1, 32, 14, 14]               0\n",
      "            Conv2d-8           [-1, 64, 14, 14]          18,432\n",
      "       BatchNorm2d-9           [-1, 64, 14, 14]             128\n",
      "             ReLU-10           [-1, 64, 14, 14]               0\n",
      "           Conv2d-11           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 14, 14]             128\n",
      "             ReLU-13           [-1, 64, 14, 14]               0\n",
      "           Conv2d-14           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 14, 14]             128\n",
      "             ReLU-16           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-17             [-1, 64, 7, 7]               0\n",
      "          Flatten-18                 [-1, 3136]               0\n",
      "           Linear-19                   [-1, 64]         200,768\n",
      "             ReLU-20                   [-1, 64]               0\n",
      "           Linear-21                   [-1, 32]           2,080\n",
      "             ReLU-22                   [-1, 32]               0\n",
      "          Dropout-23                   [-1, 32]               0\n",
      "           Linear-24                   [-1, 10]             330\n",
      "================================================================\n",
      "Total params: 305,354\n",
      "Trainable params: 305,354\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.68\n",
      "Params size (MB): 1.16\n",
      "Estimated Total Size (MB): 2.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Creating Model\n",
    "\n",
    "#Define architecture\n",
    "class MNISTModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(MNISTModel,self).__init__()\n",
    "\n",
    "\t\t#Layer1\n",
    "\t\tself.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,padding=1,bias=False)#Size does not change\n",
    "\t\tself.bn1 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.relu1 = nn.ReLU()\n",
    "\t\tself.pool1 = nn.MaxPool2d(kernel_size=2)#Size halves into 14x14\n",
    "\n",
    "\t\t#Layer2\n",
    "\t\tself.conv2 = nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.relu2 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer3\n",
    "\t\tself.conv3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.relu3 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer4\n",
    "\t\tself.conv4 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.relu4 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer5\n",
    "\t\tself.conv5 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn5 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.relu5 = nn.ReLU()\n",
    "\t\tself.pool2 = nn.MaxPool2d(kernel_size=2)#Size halves into 7x7\n",
    "\n",
    "\t\t#FlattenLayer\n",
    "\t\tself.flatten = nn.Flatten()\n",
    "\n",
    "\t\t#Layer6\n",
    "\t\tself.fc1 = nn.Linear(in_features=64*7*7,out_features=64)\n",
    "\t\tself.relu6 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer7\n",
    "\t\tself.fc2 = nn.Linear(in_features=64,out_features=32)\n",
    "\t\tself.relu7 = nn.ReLU()\n",
    "\n",
    "\t\t#DropoutLayer\n",
    "\t\tself.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "\t\t#Layer8\n",
    "\t\tself.fc3 = nn.Linear(in_features=32,out_features=10)\n",
    "\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t#Pass through Layer1\n",
    "\t\tx = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "\n",
    "\t\t#Pass through Layer2\n",
    "\t\tx = self.relu2(self.bn2(self.conv2(x)))\n",
    "\n",
    "\t\t#Pass through Layer3\n",
    "\t\tx = self.relu3(self.bn3(self.conv3(x)))\n",
    "\n",
    "\t\t#Pass through Layer4\n",
    "\t\tx = self.relu4(self.bn4(self.conv4(x)))\n",
    "\n",
    "\t\t#Pass through Layer5\n",
    "\t\tx = self.pool2(self.relu5(self.bn5(self.conv5(x))))\n",
    "\n",
    "\t\t#Pass through Layer5\n",
    "\t\tx = self.flatten(x)\n",
    "\n",
    "\t\t#Pass through Layer6\n",
    "\t\tx = self.relu6(self.fc1(x))\n",
    "\n",
    "\t\t#Pass through Layer7\n",
    "\t\tx = self.relu7(self.fc2(x))\n",
    "\n",
    "\t\t#Pass through DropoutLayer\n",
    "\t\tx = self.dropout(x)\n",
    "\n",
    "\t\t#Pass through Layer7\n",
    "\t\tx = self.fc3(x)\n",
    "\n",
    "\t\t#Return Prediction\n",
    "\t\treturn x\n",
    "\n",
    "#Create and print summary\t\n",
    "print(\"Creating MNIST Model...\")\n",
    "model = MNISTModel().to(device)\n",
    "print(\"Model Created\")\n",
    "\n",
    "print(\"Model Summary: \")\n",
    "summary(model,input_size=(1,28,28))\n",
    "\n",
    "#Compile model\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5ea1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n",
      "Epoch: 1/100 | Training Loss: 0.511 | Validation Loss: 0.273 | Accuracy: 91.063%\n",
      "Validation Loss has improved at epoch 1. Model Saved!\n",
      "Epoch: 2/100 | Training Loss: 0.245 | Validation Loss: 0.281 | Accuracy: 90.850%\n",
      "Validation Loss has not improved. Patience:1/5\n",
      "Epoch: 3/100 | Training Loss: 0.208 | Validation Loss: 0.188 | Accuracy: 93.999%\n",
      "Validation Loss has improved at epoch 3. Model Saved!\n",
      "Epoch: 4/100 | Training Loss: 0.192 | Validation Loss: 0.173 | Accuracy: 94.554%\n",
      "Validation Loss has improved at epoch 4. Model Saved!\n",
      "Epoch: 5/100 | Training Loss: 0.175 | Validation Loss: 0.165 | Accuracy: 94.793%\n",
      "Validation Loss has improved at epoch 5. Model Saved!\n",
      "Epoch: 6/100 | Training Loss: 0.165 | Validation Loss: 0.152 | Accuracy: 95.123%\n",
      "Validation Loss has improved at epoch 6. Model Saved!\n",
      "Epoch: 7/100 | Training Loss: 0.160 | Validation Loss: 0.139 | Accuracy: 95.388%\n",
      "Validation Loss has improved at epoch 7. Model Saved!\n",
      "Epoch: 8/100 | Training Loss: 0.154 | Validation Loss: 0.171 | Accuracy: 94.424%\n",
      "Validation Loss has not improved. Patience:1/5\n",
      "Epoch: 9/100 | Training Loss: 0.151 | Validation Loss: 0.151 | Accuracy: 95.050%\n",
      "Validation Loss has not improved. Patience:2/5\n",
      "Epoch: 10/100 | Training Loss: 0.144 | Validation Loss: 0.146 | Accuracy: 95.314%\n",
      "Validation Loss has not improved. Patience:3/5\n",
      "Epoch: 11/100 | Training Loss: 0.142 | Validation Loss: 0.133 | Accuracy: 95.636%\n",
      "Validation Loss has improved at epoch 11. Model Saved!\n",
      "Epoch: 12/100 | Training Loss: 0.138 | Validation Loss: 0.120 | Accuracy: 96.027%\n",
      "Validation Loss has improved at epoch 12. Model Saved!\n",
      "Epoch: 13/100 | Training Loss: 0.135 | Validation Loss: 0.129 | Accuracy: 95.705%\n",
      "Validation Loss has not improved. Patience:1/5\n",
      "Epoch: 14/100 | Training Loss: 0.133 | Validation Loss: 0.130 | Accuracy: 95.670%\n",
      "Validation Loss has not improved. Patience:2/5\n",
      "Epoch: 15/100 | Training Loss: 0.129 | Validation Loss: 0.119 | Accuracy: 96.074%\n",
      "Validation Loss has improved at epoch 15. Model Saved!\n",
      "Epoch: 16/100 | Training Loss: 0.125 | Validation Loss: 0.126 | Accuracy: 95.870%\n",
      "Validation Loss has not improved. Patience:1/5\n",
      "Epoch: 17/100 | Training Loss: 0.124 | Validation Loss: 0.118 | Accuracy: 96.200%\n",
      "Validation Loss has improved at epoch 17. Model Saved!\n",
      "Epoch: 18/100 | Training Loss: 0.123 | Validation Loss: 0.122 | Accuracy: 95.927%\n",
      "Validation Loss has not improved. Patience:1/5\n",
      "Epoch: 19/100 | Training Loss: 0.122 | Validation Loss: 0.121 | Accuracy: 96.079%\n",
      "Validation Loss has not improved. Patience:2/5\n",
      "Epoch: 20/100 | Training Loss: 0.119 | Validation Loss: 0.121 | Accuracy: 95.979%\n",
      "Validation Loss has not improved. Patience:3/5\n",
      "Epoch: 21/100 | Training Loss: 0.117 | Validation Loss: 0.113 | Accuracy: 96.300%\n",
      "Validation Loss has improved at epoch 21. Model Saved!\n",
      "Epoch: 22/100 | Training Loss: 0.119 | Validation Loss: 0.113 | Accuracy: 96.105%\n",
      "Validation Loss has improved at epoch 22. Model Saved!\n",
      "Epoch: 23/100 | Training Loss: 0.116 | Validation Loss: 0.109 | Accuracy: 96.274%\n",
      "Validation Loss has improved at epoch 23. Model Saved!\n",
      "Epoch: 24/100 | Training Loss: 0.113 | Validation Loss: 0.111 | Accuracy: 96.326%\n",
      "Validation Loss has not improved. Patience:1/5\n",
      "Epoch: 25/100 | Training Loss: 0.110 | Validation Loss: 0.118 | Accuracy: 96.087%\n",
      "Validation Loss has not improved. Patience:2/5\n",
      "Epoch: 26/100 | Training Loss: 0.112 | Validation Loss: 0.114 | Accuracy: 96.265%\n",
      "Validation Loss has not improved. Patience:3/5\n",
      "Epoch: 27/100 | Training Loss: 0.110 | Validation Loss: 0.114 | Accuracy: 96.161%\n",
      "Validation Loss has not improved. Patience:4/5\n",
      "Epoch: 28/100 | Training Loss: 0.108 | Validation Loss: 0.115 | Accuracy: 96.304%\n",
      "Validation Loss has not improved. Patience:5/5\n",
      "Early stopping triggered!\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "#Training and saving the model\n",
    "\n",
    "print(\"Training Model...\")\n",
    "epochs = 100\n",
    "patience = 5\n",
    "counter = 0\n",
    "MinValidation = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\t#Train Loop\n",
    "\n",
    "\t#Set model to training mode\n",
    "\tmodel.train()\n",
    "\tTrainLoss = 0.0\n",
    "\n",
    "\tfor i,data in enumerate(TrainLoader,0):\n",
    "\t\tinputs,labels = data[0].to(device),data[1].to(device)\n",
    "\n",
    "\t\t#Zero the parameter gradients\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t#Forward pass\n",
    "\t\toutputs = model(inputs)\n",
    "\n",
    "\t\t#Calculate loss\n",
    "\t\tloss = loss_function(outputs,labels)\n",
    "\n",
    "\t\t#Backward pass\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\t#Update weights\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t#Update loss\n",
    "\t\tTrainLoss += loss.item()\n",
    "\t\n",
    "\t#Validation Loop\n",
    "\tmodel.eval()\n",
    "\tValidationLoss = 0.0\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor data in ValLoader:\n",
    "\t\t\timages,labels = data[0].to(device),data[1].to(device)\n",
    "\t\t\toutputs = model(images)\n",
    "\t\t\tloss = loss_function(outputs,labels)\n",
    "\t\t\tValidationLoss += loss.item()\n",
    "\n",
    "\t\t\t_,predicted = torch.max(outputs.data,1)\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\t\t\tcorrect += (predicted == labels).sum().item()\n",
    "\n",
    "\t\tacc = 100*correct/total\n",
    "\t\tprint(f\"Epoch: {epoch+1}/{epochs} | \"\n",
    "\t\t\tf\"Training Loss: {TrainLoss/len(TrainLoader):.3f} | \"\n",
    "\t\t\tf\"Validation Loss: {ValidationLoss / len(ValLoader):.3f} | \"\n",
    "\t\t\tf\"Accuracy: {acc:.3f}%\")\n",
    "\t\t\n",
    "\t\tif ValidationLoss < MinValidation:\n",
    "\t\t\tMinValidation = ValidationLoss\n",
    "\t\t\tcounter = 0\n",
    "\t\t\ttorch.save(model.state_dict(),\"MNISTModel.pth\")\n",
    "\t\t\tprint(f\"Validation Loss has improved at epoch {epoch+1}. Model Saved!\")\n",
    "\t\telse:\n",
    "\t\t\tcounter += 1\n",
    "\t\t\tprint(f\"Validation Loss has not improved. Patience:{counter}/{patience}\")\n",
    "\t\t\n",
    "\t\tif counter >= patience:\n",
    "\t\t\tprint(\"Early stopping triggered!\")\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98444dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with inbuilt dataset...\n",
      "Test Accuracy: 96.309%\n",
      "Test Loss: 0.114\n"
     ]
    }
   ],
   "source": [
    "#Testing Model\n",
    "\n",
    "print(\"Testing model with inbuilt dataset...\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"MNISTModel.pth\"))\n",
    "\n",
    "TestLoss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor images, labels in TestLoader:\n",
    "\n",
    "\t\t#Move data to device\n",
    "\t\timages,labels = images.to(device),labels.to(device)\n",
    "\n",
    "\t\t#Forward pass\n",
    "\t\toutputs = model(images)\n",
    "\n",
    "\t\t#Calculate loss\n",
    "\t\tloss = loss_function(outputs,labels)\n",
    "\t\tTestLoss += loss.item()\n",
    "\n",
    "\t\t#Get predicted class\n",
    "\t\t_,predicted = torch.max(outputs.data,1)\n",
    "\n",
    "\t\t#Update total and correct counts\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t\n",
    "#Test Results\n",
    "\n",
    "FinalLoss = TestLoss/len(TestLoader)\n",
    "FinalAcc = 100*correct/total\n",
    "\n",
    "print(f\"Test Accuracy: {FinalAcc:.3f}%\")\n",
    "print(f\"Test Loss: {FinalLoss:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
